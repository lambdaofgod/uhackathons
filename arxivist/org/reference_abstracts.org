#+title: Reference_abstracts

[[/home/kuba/Projects/org/roam/20230528181338-project_ideas.org::140][20230528181338-project_ideas.org::140 (in /home/kuba/Projects/org/roam/20230528181338-project_ideas.org)]]

* Org-ai

** NO get abstract via API

#+BEGIN_AI
How to get arXiv paper abstract via API?

[AI]: You can get arXiv paper abstracts via the arXiv API by sending a GET request to the following endpoint:

```
http://export.arxiv.org/api/query?id_list=<paper_id>
```

Replace `<paper_id>` with the arXiv paper identifier you are interested in. The response will be in XML format and will contain information about the requested paper, including its abstract. 

You can use tools like cURL or libraries in programming languages like Python or JavaScript to make HTTP requests to the arXiv API and retrieve the paper abstracts.

[ME]: Can it output JSON?

[AI]: The arXiv API does not have built-in support for outputting JSON directly. However, you can convert the XML response you receive from the API into JSON format using various programming libraries that provide XML-to-JSON conversion functionalities.

For example, in Python, you can use the `xmltodict` library to convert the XML response from the arXiv API into a JSON object. Here's a simple example:

```python
import requests
import xmltodict
import json

paper_id = "your_paper_id"
url = f"http://export.arxiv.org/api/query?id_list={paper_id}"
response = requests.get(url)
data_dict = xmltodict.parse(response.content)
json_data = json.dumps(data_dict)

print(json_data)
```

By using the `xmltodict` library in the example above, you can convert the XML response from the arXiv API into JSON format for easier processing and manipulation in your code.

[ME]: The URL you provided doesn't give abstracts.

[AI]: I apologize for the oversight. The correct way to retrieve arXiv paper abstracts via the API is to use the `arXiv.org e-prints API v1.1` endpoint.

You can send a GET request to the following endpoint:

```
http://export.arxiv.org/api/query?id_list=<paper_id>&start=0&max_results=1
```

Replace `<paper_id>` with the arXiv paper identifier you are interested in. This endpoint will return metadata information about the specified paper, including the abstract, in XML format.

You can then parse the XML response and extract the abstract from it using XML parsing libraries in your preferred programming language.

[ME]: That didn't change anything. Try #+END_AI

** YES Extract abstract

#+BEGIN_AI
Write Python function `extract_abstract` to extract abstract text from HTML that contains such element:

```
    <blockquote class="abstract mathjax">
            <span class="descriptor">Abstract:</span>  In recent years, spectral clustering has become one of the most popular modern clustering algorithms. It is simple to implement, can be solved efficiently by standard linear algebra software, and very often outperforms traditional clustering algorithms such as the k-means algorithm. On the first glance spectral clustering appears slightly mysterious, and it is not obvious to see why it works at all and what it really does. The goal of this tutorial is to give some intuition on those questions. We describe different graph Laplacians and their basic properties, present the most common spectral clustering algorithms, and derive those algorithms from scratch by several different approaches. Advantages and disadvantages of the different spectral clustering algorithms are discussed.
    </blockquote>
```

[AI]: Here is a Python function `extract_abstract` that extracts the abstract text from an HTML string that contains the specified element structure:

```python
from bs4 import BeautifulSoup

def extract_abstract(html_content):
    # Parse the HTML content
    soup = BeautifulSoup(html_content, 'html.parser')
    
    # Find the blockquote element with class "abstract mathjax"
    abstract_element = soup.find('blockquote', class_='abstract mathjax')
    
    # Check if the abstract element is found
    if abstract_element:
        # Extract the text inside the abstract element
        abstract_text = abstract_element.text
        return abstract_text.strip()
    
    return None

# HTML content containing the abstract element
html_content = '''
<blockquote class="abstract mathjax">
    <span class="descriptor">Abstract:</span> In recent years, spectral clustering has become one of the most popular modern clustering algorithms. It is simple to implement, can be solved efficiently by standard linear algebra software, and very often outperforms traditional clustering algorithms such as the k-means algorithm. On the first glance spectral clustering appears slightly mysterious, and it is not obvious to see why it works at all and what it really does. The goal of this tutorial is to give some intuition on those questions. We describe different graph Laplacians and their basic properties, present the most common spectral clustering algorithms, and derive those algorithms from scratch by several different approaches. Advantages and disadvantages of the different spectral clustering algorithms are discussed.
</blockquote>
'''

# Extract the abstract text from the HTML content
abstract = extract_abstract(html_content)
print(abstract)
```

You can run this code snippet to extract the abstract text from the provided HTML content. Make sure you have the `BeautifulSoup` library installed (`pip install beautifulsoup4`) to parse the HTML content.

[ME]: #+END_AI

** Extract arXiv IDs from bibtex

#+BEGIN_AI
Write Python code that extracts citations in BibTex format given an arXiv paper ID

[AI]: To extract citations in BibTeX format for a paper given its arXiv ID, you can use the arXiv API to fetch the metadata including the reference information. You can then parse the retrieved data to extract the citation details in BibTeX format. Here is an example Python code snippet that demonstrates how to achieve this:

```python
import requests
import bibtexparser
from bibtexparser.bparser import BibTexParser
from bibtexparser.bibdatabase import BibDatabase

def get_bibtex_citations_from_arxiv(arxiv_id):
    url = f"https://export.arxiv.org/api/query?id_list={arxiv_id}"
    response = requests.get(url)
    
    if response.status_code == 200:
        data = response.text
        bib_database = BibTexParser(common_strings=True).parse(data)
        
        if bib_database.entries:
            return bib_database.to_bibtex()
        else:
            return "No citations found for this arXiv ID."
                
    else:
        return "Failed to fetch metadata from arXiv API."

# Specify the arXiv paper ID
arxiv_id = "1707.06347"

# Extract citations in BibTeX format
bibtex_citations = get_bibtex_citations_from_arxiv(arxiv_id)
print(bibtex_citations)
```

In this code snippet:
1. We define a function `get_bibtex_citations_from_arxiv` that takes an arXiv ID as input, retrieves the metadata from the arXiv API, and parses it to extract the citation details in BibTeX format.
2. We make a GET request to the arXiv API to fetch the metadata for the specified arXiv ID.
3. If the response status is 200 (OK), we parse the retrieved data using `BibTexParser` from the `bibtexparser` library.
4. We then check if any bibliography entries are found and return the citations in BibTeX format.
5. Finally, we call the function with a sample arXiv ID ("1707.06347") and print the extracted citations in BibTeX format.

You can run this code snippet in your Python environment after installing the required libraries `requests` and `bibtexparser`.

[ME]: #+END_AI

** References

#+BEGIN_AI
Write Python function that will extract elements from the following section in HTML:

```
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Arora et al. (2024)</span>
<span class="ltx_bibblock">
Daman Arora, Atharv Sonwane, Nalin Wadhwa, Abhav Mehrotra, Saiteja Utpala, Ramakrishna Bairi, Aditya Kanade, and Nagarajan Natarajan.

</span>
<span class="ltx_bibblock">Masai: Modular architecture for software-engineering ai agents.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">arXiv preprint arXiv:2406.11638</em>, 2024.
```


[AI]: You can use the BeautifulSoup library in Python to perform HTML parsing and data extraction.

The Python function to extract the elements will look something like this:

```python
from bs4 import BeautifulSoup

def extract_elements(html_section):
    soup = BeautifulSoup(html_section, 'html.parser')
    bibsection = soup.find('section', {'class': 'ltx_bibliography'})

    records = []
    bibitems = bibsection.find_all('li', {'class': 'ltx_bibitem'})
    for item in bibitems:
        record = {}
        record['id'] = item.get('id')
        record['refnum'] = item.find('span', {'class': 'ltx_tag ltx_role_refnum ltx_tag_bibitem'}).text
        bibblocks = item.find_all('span', {'class': 'ltx_bibblock'})

        # Assuming the order of span elements are same across all li elements
        record['authors'] = bibblocks[0].text.strip()
        record['title'] = bibblocks[1].text.strip()
        record['source'] = bibblocks[2].text.strip()
        
        records.append(record)
    return records
```

This function takes an HTML string as argument, parses the string to create a BeautifulSoup object, looks for `<li>` elements with `class="ltx_bibitem"` containing the bibliographic items and extracts the bibliographic details (like id,#+END_AI


** arxiv ids

#+BEGIN_AI
Write Python function extract_arxiv_id(elem) that will extract arXiv ID from the following:

```
elem = "Arora et\xa0al. (2024)\n\nDaman Arora, Atharv Sonwane, Nalin Wadhwa, Abhav Mehrotra, Saiteja Utpala, Ramakrishna Bairi, Aditya Kanade, and Nagarajan Natarajan.\n\n\nMasai: Modular architecture for software-engineering ai agents.\n\n\narXiv preprint arXiv:2406.11638, 2024.\n\n\n"
```

[AI]: Sure, you can use Python's regex module (`re`) to search for the arXiv ID pattern in the string. Here's how you might define the function `extract_arxiv_id(elem)`:

```python
import re

def extract_arxiv_id(elem):
    match = re.search(r'arXiv:(\d+\.\d+)', elem)
    if match:
        return match.group(1)
    else:
        return None
```

In this function, `re.search(r'arXiv:(\d+\.\d+)', elem)` looks for strings that start with "arXiv:", followed by a sequence of digits, a dot, and another sequence of digits. The parentheses in `(\d+\.\d+)` indicate that this part of the pattern is a group â€” this allows the `.group(1)` function to extract the part of the match that this group corresponds to.

Here's how you might use this function:

```python
elem = "Arora et\xa0al. (2024)\n\nDaman Arora, Atharv Sonwane, Nalin Wadhwa, Abhav Mehrotra, Saiteja Utpala, Ramakrishna Bairi, Aditya Kanade, and Nagarajan Natarajan.\n\n\nMasai: Modular architecture for software-engineering ai agents.\n\n\narXiv preprint arXiv:2406.11638, 2024.\n\n\n"
arxiv_id = extract_arxiv_id(elem)
print(f'Extracted arXiv ID: {arxiv_id}')
``#+END_AI

* Code

#+BEGIN_SRC python :session reference_abstracts.org  :exports both
from pathlib import Path
from arxivist.reference_abstracts import ReferenceAbstractExtractor

with open(Path("~/").expanduser() / ".keys/perplexity_api_key.txt") as f:
    PERPLEXITY_API_KEY = f.read().strip()
#+END_SRC

#+RESULTS:

#+BEGIN_SRC python :session reference_abstracts.org  :exports both

paper_id = "2408.03910"
abs_url = f"http://arxiv.org/abs/{paper_id}"
html_url = f"http://arxiv.org/html/{paper_id}"

abs_response = requests.get(abs_url)
html_response = requests.get(html_url)
abstract = extract_abstract(abs_response.content)
#+END_SRC

#+RESULTS:

#+BEGIN_SRC python :session reference_abstracts.org  :exports both :async
referenced_abstracts = ReferenceAbstractExtractor.get_referenced_paper_abstracts(paper_id)
#+END_SRC

#+RESULTS:

#+BEGIN_SRC python :session reference_abstracts.org  :exports both
abstract
#+END_SRC

#+RESULTS:
: Large Language Models (LLMs) excel in stand-alone code tasks like HumanEval and MBPP, but struggle with handling entire code repositories. This challenge has prompted research on enhancing LLM-codebase interaction at a repository scale. Current solutions rely on similarity-based retrieval or manual tools and APIs, each with notable drawbacks. Similarity-based retrieval often has low recall in complex tasks, while manual tools and APIs are typically task-specific and require expert knowledge, reducing their generalizability across diverse code tasks and real-world applications. To mitigate these limitations, we introduce \framework, a system that integrates LLM agents with graph database interfaces extracted from code repositories. By leveraging the structural properties of graph databases and the flexibility of the graph query language, \framework enables the LLM agent to construct and execute queries, allowing for precise, code structure-aware context retrieval and code navigation. We assess \framework using three benchmarks: CrossCodeEval, SWE-bench, and EvoCodeBench. Additionally, we develop five real-world coding applications. With a unified graph database schema, \framework demonstrates competitive performance and potential in both academic and real-world environments, showcasing its versatility and efficacy in software engineering. Our application demo: this https URL.

#+BEGIN_SRC python :session reference_abstracts.org  :exports both
bibsections = extract_references(html_response.content)
#+END_SRC

#+RESULTS:

#+BEGIN_SRC python :session reference_abstracts.org  :exports both
import re

arxiv_id = "2408.03910"

ReferenceAbstractExtractor.get_referenced_arxiv_ids(arxiv_id)
#+END_SRC

#+RESULTS:
| 2406.11638 | 2108.07732 | 2107.03374 | 2405.19782 | 2406.01359 | 2404.16130 | 2203.03850 | 2402.07630 | 2308.00352 | 2406.00515 | 2310.06770 | 2406.04464 | 2403.08604 | 2404.00599 | 2406.02110 | 2306.03091 | 2406.07003 | 2406.01422 | 2403.06095 | 2307.07924 | 2306.10998 | 2403.14734 | 2402.01030 | 2407.01489 | 2312.17449 | 2405.15793 | 2403.16443 | 2303.12570 | 2404.05427 | 2406.11931 |
