#+title: Sae Experiments

* Decoder matrix orthogonality
:PROPERTIES:
:CREATED:  <2025-09-06 Sat> [15:45]
:END:

This experiment analyzes sparse autoencoders from the perspective of sparse coding. Specifically we want to investigate the properties of decoder matrix typically used for the last step in SAEs. The idea comes from the fact that from its sheer size we expect it to be almost orthogonal.

** Intro

Typically SAE training requires specifying some hyperparameter to control the sparsity. The problem is that this is then baked in the model. For example the /sae_lens/ config in /sae_lens/pretrained_saes.yaml/

Because the SAE decoder is typically based just on matrix multiplication $\hat{x} = M_{dec} z  + b_{dec}$, we can use a different approach inspired by dictionary learning. The simplest approach would be to actually ignore the encoder and just find sparsest $z$ (using "l_0 norm", $|z|_0 = |{i: |z_i| > 0}|$) that minimizes the reconstruction error, specifically $z = z(s)$

$\|M_{dec} z  + b_{dec}\|^2, \textit{ such that } |z|_0 \leq s$

In general this is a hard optimization problem, but there exists rich literature on sparse representations and compressed sensing that analyzes similar situations - in some situations this optimization problem is actually guaranteed to have unique solutions that can be found with simple methods.

** Controlling sparsity

- most SAEs don't do that directly, or do it using a hyperparameter that interacts with nonlinearity nontrivially (see how many versions there are for gemma-2)
- usual decoding sort of lets us do that but with no guarantees
- it would be great if we were able to have a simple procedure to update $z(s)$ once we increase $s$
- this is where sparse representations come in

** Some facts about sparse representations

- restricted isometry property & friends - guarantee to find sparsest solutions that minimize reconstruction error
- RIP & friends are in general NP-hard to compute
- mutual coherence is extremely simple and can be used for bounding RIP
- if the matrices are close to orthogonal then greedy decoding algorithms are guaranteed to find optimal solutions
- the exact condition is for $s$-sparse vectors simple algorithms will work if $(s-1)\mu_(M)$ is small enough
- if the matrix is actually orthogonal then extremely simple algorithm can be used to find the solution

** Experiment: gemma-scope SAE for gemma-2-2b
:PROPERTIES:
:CREATED:  <2025-09-06 Sat> [15:46]
:END:

model = "gemma-2-2b"
release="gemma-scope-2b-pt-res"
sae_id="layer_21/width_16k/average_l0_301"


#+BEGIN_SRC python :session SAE_EXPERIMENTS.org :async  :exports both
# -*- coding: utf-8 -*-
"""SAE Lens basic_loading_and_analysing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1assS27KybAFvJ4gdKtntybTZVgf38l3U

# Loading and Analysing Pre-Trained Sparse Autoencoders

## Imports & Installs
"""

# Standard imports
import seaborn as sns
import matplotlib.pyplot as plt
import os
import torch
from tqdm import tqdm
import plotly.express as px

torch.set_grad_enabled(False)

"""## Set Up

"""

# For the most part I'll try to import functions and classes near where they are used
# to make it clear where they come from.

if torch.backends.mps.is_available():
    device = "mps"
else:
    device = "cuda" if torch.cuda.is_available() else "cpu"

print(f"Device: {device}")

from huggingface_hub import notebook_login

from datasets import load_dataset
from transformer_lens import HookedTransformer
from sae_lens import SAE

model = HookedTransformer.from_pretrained("gemma-2-2b", device=device)

# the cfg dict is returned alongside the SAE since it may contain useful information for analysing the SAE (eg: instantiating an activation store)
# Note that this is not the same as the SAEs config dict, rather it is whatever was in the HF repo, from which we can extract the SAE config dict
# We also return the feature sparsities which are stored in HF for convenience.
sae = SAE.from_pretrained(
    release="gemma-scope-2b-pt-res",  # see other options in sae_lens/pretrained_saes.yaml
    sae_id="layer_21/width_16k/average_l0_301",  # won't always be a hook point
    device=device,
)

import numpy as np
import sklearn

modules = sae.mod_dict

M_Decoder = model.unembed.W_U.cpu().numpy()
M_Decoder.shape
Gram_Decoder = sklearn.metrics.pairwise.cosine_similarity(M_Decoder)
np.fill_diagonal(Gram_Decoder, np.zeros(Gram_Decoder.shape[0]))
Gram_Decoder

Gram_Decoder.max()

import pandas as pd

coherences = pd.Series(Gram_Decoder.reshape(-1))
print("Coherences of decoder matrix statistics")
print(coherences.describe())
#+END_SRC

#+RESULTS:
: None

The problem here is this

#+BEGIN_SRC python :session SAE_EXPERIMENTS.org  :exports both
coherences.describe()
#+END_SRC

#+RESULTS:
: count    5.308416e+06
: mean    -3.033965e-05
: std      8.154385e-02
: min     -9.677635e-01
: 25%     -2.915314e-02
: 50%     -4.839342e-06
: 75%      2.916406e-02
: max      9.592159e-01
: dtype: float64

#+BEGIN_SRC python :session SAE_EXPERIMENTS.org  :exports both
coherences.quantile([0.8, 0.85, 0.9, 0.95])
#+END_SRC

#+RESULTS:
: 0.80    0.039294
: 0.85    0.053579
: 0.90    0.076147
: 0.95    0.119957
: dtype: float64

** Sources
:PROPERTIES:
:CREATED:  <2025-09-06 Sat> [16:14]
:END:

- Elad's 5 lectures on sparse representations
- SAE lens
- Mathematical introduction to compressed sensing
- Statistical learning with sparsity

Stuff to quote:
Gemma scope
