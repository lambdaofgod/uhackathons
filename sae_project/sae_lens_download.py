# -*- coding: utf-8 -*-
"""SAE Lens basic_loading_and_analysing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1assS27KybAFvJ4gdKtntybTZVgf38l3U

# Loading and Analysing Pre-Trained Sparse Autoencoders

## Imports & Installs
"""

# Standard imports
import seaborn as sns
import matplotlib.pyplot as plt
import os
import torch
from tqdm import tqdm
import plotly.express as px

torch.set_grad_enabled(False)

"""## Set Up

"""

# For the most part I'll try to import functions and classes near where they are used
# to make it clear where they come from.

if torch.backends.mps.is_available():
    device = "mps"
else:
    device = "cuda" if torch.cuda.is_available() else "cpu"

print(f"Device: {device}")

from huggingface_hub import notebook_login

from datasets import load_dataset
from transformer_lens import HookedTransformer
from sae_lens import SAE

model = HookedTransformer.from_pretrained("gemma-2-2b", device=device)

# the cfg dict is returned alongside the SAE since it may contain useful information for analysing the SAE (eg: instantiating an activation store)
# Note that this is not the same as the SAEs config dict, rather it is whatever was in the HF repo, from which we can extract the SAE config dict
# We also return the feature sparsities which are stored in HF for convenience.
sae = SAE.from_pretrained(
    release="gemma-scope-2b-pt-res",  # see other options in sae_lens/pretrained_saes.yaml
    sae_id="layer_21/width_16k/average_l0_301",  # won't always be a hook point
    device=device,
)

import numpy as np
import sklearn

modules = sae.mod_dict

M_Decoder = model.unembed.W_U.cpu().numpy()
M_Decoder.shape
Gram_Decoder = sklearn.metrics.pairwise.cosine_similarity(M_Decoder)
np.fill_diagonal(Gram_Decoder, np.zeros(Gram_Decoder.shape[0]))
Gram_Decoder

Gram_Decoder.max()

import pandas as pd

coherences = pd.Series(Gram_Decoder.reshape(-1))
print("Coherences of decoder matrix statistics")
print(coherences.describe())


sns.distplot(coherences)
plt.show()
